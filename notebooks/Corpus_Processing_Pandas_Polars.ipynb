{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b1d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus_Processing_Pandas_Polars.ipynb\n",
    "\n",
    "# Import necessary libraries\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Define paths to your corpus folders\n",
    "# Replace the below dummy paths with the actual folder paths where your XMI files are located\n",
    "cdlk_path = r\"/path/to/CDLK/learner_xmi\"  # e.g., \"/home/user/data/CDLK/learner_xmi\"\n",
    "klp1_path = r\"/path/to/KLP1/learner_xmi\"  # e.g., \"/home/user/data/KLP1/learner_xmi\"\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Define functions to parse XMI files and extract basic info\n",
    "\n",
    "def parse_xmi_basic_info(xmi_file):\n",
    "    \"\"\"\n",
    "    Parses an XMI file to extract basic corpus statistics:\n",
    "    - document text length\n",
    "    - token count\n",
    "    - sentence count\n",
    "    - annotation count\n",
    "\n",
    "    Parameters:\n",
    "        xmi_file (Path): Path object to the XMI file\n",
    "\n",
    "    Returns:\n",
    "        dict: Parsed information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(xmi_file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Extract document text length from 'Sofa' element attribute 'sofaString'\n",
    "        doc_text_length = None\n",
    "        for sofa in root.findall(\".//{*}Sofa\"):\n",
    "            text = sofa.attrib.get(\"sofaString\")\n",
    "            if text:\n",
    "                doc_text_length = len(text)\n",
    "                break\n",
    "\n",
    "        # Count tokens, sentences, and annotations based on tag names\n",
    "        token_count = sum(1 for elem in root.iter() if \"Token\" in elem.tag)\n",
    "        sentence_count = sum(1 for elem in root.iter() if \"Sentence\" in elem.tag)\n",
    "        annotation_count = sum(1 for elem in root.iter() if \"Annotation\" in elem.tag)\n",
    "\n",
    "        return {\n",
    "            \"filename\": xmi_file.name,\n",
    "            \"filepath\": str(xmi_file),\n",
    "            \"doc_text_length\": doc_text_length,\n",
    "            \"token_count\": token_count,\n",
    "            \"sentence_count\": sentence_count,\n",
    "            \"annotation_count\": annotation_count,\n",
    "        }\n",
    "    except ET.ParseError:\n",
    "        # Return None if parsing fails\n",
    "        return {\n",
    "            \"filename\": xmi_file.name,\n",
    "            \"filepath\": str(xmi_file),\n",
    "            \"doc_text_length\": None,\n",
    "            \"token_count\": None,\n",
    "            \"sentence_count\": None,\n",
    "            \"annotation_count\": None,\n",
    "        }\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Define processing functions for Pandas and Polars\n",
    "\n",
    "def process_corpus_folder_pandas(folder_path):\n",
    "    \"\"\"\n",
    "    Process all XMI files in folder using pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        folder_path (str): Path to corpus folder\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame\n",
    "    \"\"\"\n",
    "    folder = Path(folder_path)\n",
    "    files = list(folder.glob(\"*.xmi\"))\n",
    "    records = []\n",
    "    for f in files:\n",
    "        info = parse_xmi_basic_info(f)\n",
    "        records.append(info)\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def process_corpus_folder_polars(folder_path):\n",
    "    \"\"\"\n",
    "    Process all XMI files in folder using polars DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        folder_path (str): Path to corpus folder\n",
    "\n",
    "    Returns:\n",
    "        polars.DataFrame\n",
    "    \"\"\"\n",
    "    folder = Path(folder_path)\n",
    "    files = list(folder.glob(\"*.xmi\"))\n",
    "    records = []\n",
    "    for f in files:\n",
    "        info = parse_xmi_basic_info(f)\n",
    "        records.append(info)\n",
    "    return pl.DataFrame(records)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Process corpora using both Pandas and Polars (for demonstration)\n",
    "\n",
    "print(\"Processing CDLK corpus with Pandas...\")\n",
    "df_cdlk_pd = process_corpus_folder_pandas(cdlk_path)\n",
    "print(df_cdlk_pd.head())\n",
    "\n",
    "print(\"\\nProcessing KLP1 corpus with Pandas...\")\n",
    "df_klp1_pd = process_corpus_folder_pandas(klp1_path)\n",
    "print(df_klp1_pd.head())\n",
    "\n",
    "print(\"\\nProcessing CDLK corpus with Polars...\")\n",
    "df_cdlk_pl = process_corpus_folder_polars(cdlk_path)\n",
    "print(df_cdlk_pl.head())\n",
    "\n",
    "print(\"\\nProcessing KLP1 corpus with Polars...\")\n",
    "df_klp1_pl = process_corpus_folder_polars(klp1_path)\n",
    "print(df_klp1_pl.head())\n",
    "\n",
    "# -----------------------------\n",
    "# Step 5: Save processed data to CSV for later analysis or sharing\n",
    "\n",
    "df_cdlk_pd.to_csv(\"xmi_basic_info_cdlk_pandas.csv\", index=False)\n",
    "df_klp1_pd.to_csv(\"xmi_basic_info_klp1_pandas.csv\", index=False)\n",
    "\n",
    "df_cdlk_pl.write_csv(\"xmi_basic_info_cdlk_polars.csv\")\n",
    "df_klp1_pl.write_csv(\"xmi_basic_info_klp1_polars.csv\")\n",
    "\n",
    "print(\"\\nData saved as CSV files. You can use these CSVs for further analysis.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
